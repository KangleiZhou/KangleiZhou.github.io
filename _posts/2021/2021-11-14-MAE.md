---
layout: post
category: DL
title: 【MAE】一起来读Masked Autoencoders
tagline: by Kanglei Zhou
author: Kanglei Zhou
tags: 
  - Contrastive learning
published: true
---

[微信公众号阅读效果更佳](https://mp.weixin.qq.com/s?__biz=MzUyMTE2NDYxMQ==&mid=2247489801&idx=1&sn=68c613b62d39b0b387676325b6ea260b&chksm=f9de1b25cea9923350841a90ae2d8637feb2700fa3be70978f28519bd56506a6c0f01689bc80&token=514611700&lang=zh_CN#rd)


>何恺明最新一作文章MAE火了：
>
>作者：Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, Ross Girshick
>
>机构：Facebook AI Research (FAIR)
>
>论文链接：[https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf "MAE")

作者提出了一种计算机视觉领域内可扩展的掩码自编码器（MAE，Masked Autoencoder）。MAE设计非常简单，**随机遮挡图像块并重建缺失像素**。

![MAE架构](https://files.mdnice.com/user/3650/3907ec72-1459-40b4-ac36-4079576b7e43.png)

基于两个核心设计：

- 设计一种非对称的encoder-decoder结构：
  - encoder输入为未被遮挡的图像块
  - 一个轻量化的decoder输入为可见块的隐表示何遮挡
- 高比例地遮挡（例如，75%）输入图像能够产生一个有意义的自监督任务。

基于这两点，因为编码器只输入未被mask的部分，数据量少，可加速模型训练（$3\times$）；提高准确率；因其可扩展训练大模型而泛化性好。 

仅使用 ImageNet-1K 数据，vanilla ViT-Huge 模型实现了最佳准确率 87.8%，超过了之前所有只使用 ImageNet-1K 数据的结果。并且其下游任务的迁移也优于监督训练，证明可扩展能力很可观。

# Introduction

随着算力和存储容量的不断提升，使用一百万张图像很容易让模型过拟合，但数以亿计标注好的数据集却是来之不易的。

对此，自然语言处理（NLP）领域已经有解决缺少数据集的解决方案——**自监督预训练模型**。 在自回归语言模型（如GPT）和自编码语言模型（如BERT）中，都mask一部分数据，而后让模型预测被mask部分的内容。这些方法如今可训练包含超过一千亿个参数的可泛化 NLP 模型。 

MAE是一种更通用的去噪自动编码器的形式，其在计算机视觉中也很自然且适用。实际上，在BERT出现之前，计算机视觉领域就有相关研究；而今，在BERT取得出色成绩之后，计算机视觉中的自编码方法的进展却落后于NLP。**试问MAE在视觉和语言中的差别在何处**？ 作者尝试从以下几个方面回答这个问题：

- **架构不同**：CV中卷积的设计通常容易处理欧氏空间的网格化数据，但是在处理mask单元或者位置嵌入时不是很直接。这个问题已经被Vision Transformer（ViT）解决。
- **信息密度不同**：语言是人类产生的高语音何高信息密度的信号，在预测句子中缺失单词时能够诱导复杂的语言理解。**图像是高空间冗余的自然信息**，缺失的子块可通过邻近子块恢复出来。为此，高比例地随机mask图像块能够极大降低冗余并派生一个具有挑战的自监督任务，需要对低级图像统计之外的整体进行理解。

![Mask 重建效果](https://files.mdnice.com/user/3650/3def71e2-b330-431a-9dfc-2f4aa2a15d1f.png)

- **解码器重建文本和图像作用不同**： 在视觉中，解码器重建像素，因此其输出的语义级别低于普通识别任务。这与语言相反，在语言中，解码器预测包含丰富语义信息的缺失词。虽然在BERT中，解码器可能是非常简单的（即，一个MLP），但作者发现，对于图像，解码器设计在确定学习到的潜在表示的语义水平方面起着关键作用。

基于上述分析，作者提出了一种简单、高效且可扩展的掩码自编码器MAE，用于视觉表示学习。该算法从输入图像中屏蔽随机块，并在像素空间中重建缺失块。

# Approach

## Masking

按照ViT，作者将图像划分为规则的非重叠patch。然后对一个子集的patch进行采样，并移除剩余的patch。作者的采样策略很简单：在不替换的情况下，按照均匀分布对随机patch进行采样，称之为“随机抽样”（“random sampling”）。

使用高masking ratio（即移除的patch的比率）的随机采样在很大程度上消除了冗余，因此创建了一个无法通过从可见的相邻patch进行外推能够轻松解决的任务。均匀分布可防止潜在的中心偏移（即，图像中心附近有更多mask patch）。最后，高度稀疏的输入为设计高效编码器创造了可能。

## MAE encoder

本文的编码器是ViT，但仅用于可见的、无mask的patch。就像在标准ViT中一样，本文的编码器通过线性投影获得token，并加入位置编码，然后通过一系列Transformer块处理。**不同的是，本文的编码器只需要在整个集合的一小部分（例如25%）上运行** 。这使作者能够用一小部分的计算和显存来训练非常大的编码器。

## MAE decoder

MAE解码器的输入是由编码的可见token和mask token组成的全套token。每个mask token都是一个共享的学习向量，表示要预测的缺失patch。作者向这个完整集合中的所有token添加位置嵌入；如果不加入这一点，mask token将没有关于其的位置信息。

MAE解码器仅在预训练期间用于执行图像重建任务（仅编码器用于生成用于识别的图像表示）。因此，解码器架构可以以独立于编码器设计的方式灵活设计。作者用非常小的解码器进行实验，比编码器更窄、更浅。例如，默认解码器处理每个token的计算量为编码器的10%以下。通过这种非对称设计，全套token仅由轻量级解码器处理，这大大减少了预训练时间。

## Reconstruction target

本文的MAE通过预测每个mask patch的像素值来重建输入。解码器输出中的每个元素都是表示patch的像素值向量。解码器的最后一层是一个线性投影，其输出通道的数量等于patch中像素值的数量。解码器的输出被reshape以形成重构图像。计算像素空间中重建图像和原始图像之间的损失函数为**均方误差（MSE）** 。并只计算mask patch上的损失，类似于BERT。

作者还研究了一种变体，其重建目标是每个mask patch的归一化像素值。具体来说，计算一个patch中所有像素的平均值和标准差，并使用它们来normalize这个patch。在实验中，使用归一化像素作为重建目标提高了表示质量。

## Simple implementation

MAE预训练可以有效实现，而且重要的是，不需要任何专门的稀疏操作。首先，为每个输入patch生成一个token（通过带添加位置嵌入的线性投影）。接下来，随机shuffle这个token列表，并根据masking ratio移除列表的最后一部分。此过程为编码器生成一小部分token，相当于在不替换的情况下采样了token。

编码后，作者将mask token列表附加到已编码patch列表中，并unshuffle整个列表，以将所有token与其目标对齐。解码器应用于这个完整列表（添加了位置嵌入）。如前所述，不需要稀疏操作。这个简单的实现引入了可忽略不计的计算开销，因为shuffle和unshuffle操作速度很快。

# ImageNet Experiments

## Main Properties

我们在ImageNet-1K数据集上进行自监督预训练，然后再通过监督训练评估预训练模型的表达能力。 

![Ablation study](https://files.mdnice.com/user/3650/49364a7c-2b5c-4ad0-9b7e-b99b27120db9.png)

- 由(a)和(b)，作者选择默认解码器深度为8，宽度为512维。

- 由(c)可知，编码器输入仅选择未被mask的值相比于包含被mask的值而言，性能更好。

- 由(d)可知，引入块归一化以后，模型精度可进一步提升。

- 由(e)可知，MAE仅需crop即可表现非常好，添加ColorJitter反而会影响性能 。当不使用数据增广时，MAE性能也非常优秀 。

- 由(f)可知，mask策略选择随机采样则具有最佳性能。 

![Mask sampling strategy](https://files.mdnice.com/user/3650/746b5a6a-1204-4b13-93dd-d6eeca34a1dc.png)

## Masking ratio

当掩码比例为75%时，两种方法fine-tuning和linear probing均有不错的效果。这个数值远高于BERT的15%的mask比例。 

![Masking ratio](https://files.mdnice.com/user/3650/6e207634-d89a-4342-bf0a-215c50d4ec83.png)

## Training schedules

事实上，即使在训练1600代后，也没有观察到精度的饱和。

![Training schedules](https://files.mdnice.com/user/3650/ca737a0a-c23e-498e-b40b-30bb75f7b239.png)

## Comparisons with self-supervised methods

以下展示了和SOTA方法的对比。

![Comparsion results](https://files.mdnice.com/user/3650/e1b06fc6-7d14-4e2e-a3fd-0c65a451e6cc.png)

# Transfer Learning Experiments

对迁移学习在目标检测和分割中的应用进行了评价，并对迁移学习在语义分割中的应用进行了评价。

![Transfer learning](https://files.mdnice.com/user/3650/9d3c52d4-940e-42a5-b6cf-85af9cb55db7.png)

# Conclusion

扩展性好的简单算法是深度学习的核心。在NLP中，简单的自监督学习方法可以指数级别的增益模型。在计算机视觉中，尽管在自监督学习方面取得了进展，但实际的预训练模式仍主要受到监督。在这项研究中，作者在ImageNet和迁移学习中观察到，自动编码器（autoencoder）提供了非常强的优势。视觉中的自监督学习现在可能正走上与NLP类似的轨道。

另一方面，作者注意到，图像和语言是不同性质的信号，必须仔细处理这种差异。图像仅仅是记录下来的光，没有语义分解成文字的视觉模拟。因此，作者没有尝试删除对象，而是删除最有可能不构成语义段的随机patch。同样，MAE重建了像素，而像素不是语义实体。