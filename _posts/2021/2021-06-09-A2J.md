---
layout: post
category: NLP
title: 【姿态估计】A2J: Anchor-to-Joint Regression Network
tagline: by Kanglei Zhou
author: Kanglei Zhou
tags: 
  - NLP
published: true

---



> 深度图作为一种 2.5D 的数据，其本身的深度就包含有 3D 信息。与 3D CNN 相比，2D CNN 计算量小、效率高，能够预训练。且 2D CNN 可以提取深度图的 3D 信息，使用 2D CNN 进行 3D 姿态估计是研究者们研究的热点。

# Motivation

为了从深度图中进行手部或人体姿态估计，研究者们提出了一种新的基于锚（Anchor-based）的回归网络（A2J）进行端到端的学习。在A2J中，在深度图像上密集设置能够捕获全局-局部空间上下文信息的锚点作为关节的局部回归量。它们有助于对关节的位置进行整体预测，提高泛化能力。

A2J 算法的核心思想是通过聚合多个锚点的估计结果来预测三维关节位置，以集成学习的思想提高泛化能力。

![The main idea of anchor-based 3D pose estimation paradigm within A2J.](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623156329663-image.png)

可以看到，通常不同的关节具有不同的信息性锚点。更重要的是，可见的食指指尖关节包含的信息锚点很少。而看不可见的食指中部和相对平坦区域的手掌掌心关节具有较多的信息锚点，以捕捉更丰富的空间上下文关系。

# A2J

A2J 网络由3个分支组成，由 2D CNN 骨干网驱动。这 3 个分支分别负责预测锚点与节点之间的平面偏移量、估算节点深度值和提供锚点候选信息。

![The main technical pipeline of A2J.](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623158949159-image.png)

## Backbone network architecture

骨干网络采用 ImageNet 上预训练的 ResNet-50，前 4 层对应回归 `common trunk`，第 5 层对应 `regression trunk`。本文中，做一些修改来进行 ResNet-50更适合姿态估计。

![Raw ResNet-50 architecture](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623160014322-image.png)

## In-plain offset and depth estimation branches

本质上，这两个分支旨在预测关节的三维位置。它们都包含4个256通道的中间卷积层，以及 1 个输出卷积层。

![In-plain offset and depth estimation branches](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623160451204-image.png)

输入深度图经过主干网络输出 $\times 16$ 的下采样，原来深度图中锚点的步长为 $S_t = 4$。

![The densely set anchor points on depth image. They will serve for predicting the positions of all joints in ensemble way.](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623161222656-image.png)

因此，一个特性图对应于深度图像上的 $4\times 4 = 16$ 个锚点。然后，以列的方式向所有 16 个相应的定位点设置一个具有平面尺寸特征图的输出转换层。

## Anchor proposal branch

该分支通过权重分配发现特定关节的信息性锚点，锚候选分支建立在骨干网内公共主干的输出特征图上，涉及较细的特征。

![Anchor proposal branch with 4 intermediate convolutional layers with 256 channels, and 1 output convolutional layer with $16\times K \times 1$ channels. ](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623161557897-image.png)

# Experiments

在 3 个手部数据集和 2 个人体数据集上的实验验证了 A2J 的优越性。

![Effectiveness of anchor point surrounding loss. Grey dot denotes anchor point. Red dot indicate informative anchor point. Green arrow represent in-plain offset. Yellow square corresponds to ground-truth joint.](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-8/1623161978449-image.png)

详情点击[【阅读原文】](https://mp.weixin.qq.com/s?__biz=MzUyMTE2NDYxMQ==&mid=2247489348&idx=1&sn=75e460390d3c03ba9baaddbd1abd66db&chksm=f9de1568cea99c7ecfc40d5bb1b2b38b58fa9a093fab549f130f82f184b088ee06fe49bb383f&token=2040274357&lang=zh_CN#rd)查看论文。

# 小结

通过在深度图像上密集设置 A2J 内的锚点，获取全局-局部空间上下文信息，集成预测关节位置。A2J 没有使用上采样方法，因此在一定程度上比同类方法更加有效。未来的工作，寻求更有效的方法来融合锚点。

