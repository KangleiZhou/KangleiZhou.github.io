---
layout: post
category: GPU
title: 支持“千万”数据的K-Means算法——K-Means和CUDA 
tagline: by Yunfei
tags: 
  - GPU
published: true

---

# 引言

GeForce 8800 GTX是第一个基于NVIDIA的CUDA架构构建的GPU。在它之前的图形处理架构中，只有两个可编程处理器，分别为顶点着色器和像素着色器，主要为了满足用户自定义图像应用需求。


![GeForce 8800 GTX 流水线](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616657213549-image.png)


为了使图形处理器能够用于通用计算，NVIDIA提出了CUDA架构，它包含了一个统一的着色器流水线，使得通用计算程序能够对芯片上的每个ALU进行排列。


![NVIDIA 架构](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616657314613-image.png)

# GPU与CPU区别

- CPU和GPU都具有DRAM、多级Cache、Control控制器和Core（ALU）。
- 资源分配不同导致CPU和GPU具有不同特点。


![CPU和GPU的芯片资源分布图](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616657411659-image.png)


CPU具有：

1. 强大的ALU（单核速度快） 
2. 较大的Cache（降低数据获取时延） 
3. 复杂的逻辑控制单元（支持分支预测、数据预取等复杂逻辑）

GPU具有：

1. 大量的ALU（并行能力强） 
2. 很小的Cache（用于合并多线程访问DRAM操作，即数据转发） 
3. 简单的逻辑控制单元（无上述功能）

综上所述，GPU适合简单同时**执行大量简单逻辑运算**。
保证GPU运行效率的条件：将任务分摊到更多的计算单元上、让数据离ALU更近、减少主存和显存之间的数据交换。

# CUDA C 编程语言

为了避免用户将他们的计算任务伪装成图形问题，进而需要使用面向图形的着色语言（OpenGL的GLSL），NVIDIA提供了编程语言CUDA C作为GPU的编程语言。

在编写代码前，通常需要了解设备信息以编写出性能更高的代码：

```c
cudaGetDeviceCount() // 获取设备数目
cudaGetDeviceProperties() // 获取设备信息
```

# CUDA 线程层次结构

- 内核C语言函数：调用此函数时，会由N个不同的CUDA线程并行执行N次。

- 线程（Thread）：执行核函数的单位。
- 线程块（Block）：多个线程组成线程块。一个块内的线程可彼此协作，通过shared memory来共享数据，通过`__syncthreads()`同步线程执行来协调存储器访问。


![kernel 向量加 `vecAdd`](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616657994474-image.png)

- 线程格（Grid）：启动核函数时，我们将并行执行的线程块数目指定为N，则这N个线程块组成了一个线程格。


![Host-Device 架构](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658082074-image.png)

# CUDA 存储层次结构
- 每个线程都有一个私有的本地存储器（local memory）。
- 每个线程块都有一个共享存储器（shared memory）。该存储器对于块内的所有线程都是可见的，并且与块具有相同的生命周期。
- 所有线程都可访问同一个全局存储器（global memory）。
- 两个只读的存储器，可由所有线程访问，这两个空间是常量存储器（constant memory）和纹理存储器（texture memory）。

![CUDA 存储层次结构](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658163750-image.png)


# CUDA 执行流程
- CUDA根据C代码运行的环境，分为了主机（host）和设备（device）。
- 主机和设备分别维护自己的DRAM，即host memory和device memory。
- 主机指针只能访问主机内存，设备指针只能访问设备内存，因此主机与设备之间传递数据需要拷贝。


![CUDA 执行流程](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658222522-image.png)

# 基于 CUDA 的 K-Means 算法

**简述举例算法的目标**：将多个点的按照速度的标量划分为两类


## K-Means 算法

基于欧式距离的聚类算法，该算法认为两个目标的距离越近，则其相似度越大。

算法步骤：

1. 选择初始化的 $k$ 个样本作为初始聚类中心，如 $a_1,a_2,a_3, \cdots$。
2. 针对数据集中每个样本$x_i$，计算它到 $k$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
3. 针对每个类别j，重新计算其中心：
$$𝑎_𝑗= \frac{1}{|𝑐_𝑗|}  \sum_{𝑥\in 𝑐_𝑗} 𝑥$$
4. 重复2、3步骤直至收敛。

容易想到：
1. “计算到中心的距离”：每个样本点各自独立，可以并行。
2. “每个类别内部样本求和”：无法并行，如何提高效率。

## 实现步骤

基于 CUDA 的 K-Means 算法实现大致步骤：

1. 声明设备指针（设备用于保存主机上相关数据、设备用于存储中间数据）


![声明设备指针](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658776402-image.png)


![声明其它指针](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658793978-image.png)


2. 开辟设备内存


![分配指针](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658946387-image.png)


![需要思考开多少空间](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616658981375-image.png)





3. 数据拷贝、设备内存初始化





![数据拷贝](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659038341-image.png)

![初始化内存](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659080285-image.png)


4. 运行速度矢量转标量的核函数

![运行速度矢量转标量的核函数](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659120777-image.png)


5. 运行簇分配核函数（对应K-Means算法第2步）

![运行簇分配核函数](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659144194-image.png)

如果需要使用共享内存，则在核函数中要指明开多大的空间。

6. 运行簇中心计算核函数（对应K-Means算法第3步）


![“每个类别内部样本求和”：无法并行，
初步解决方法：使用原子操作（慢！）
](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659210010-image.png)


7. 循环5、6步直至收敛
8. 将设备上的数据结果拷贝回主机内存

## 归约（Reduction）

对一个输入数组执行某种计算，然后产生一个更小的结果数组。并行计算中经常会遇到归约问题。


![归约](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659299767-image.png)


### 利用共享内存实现快速归约

- 优点：
1. 求和过程并行化，归约过程为对数时间复杂度。
2. 在共享内存上规约，读写速度快。
- 缺点：
1. 编程较复杂，需要考虑归约次数、内存分配大小、地址偏移等问题。
2. 增加了内存消耗。
3. 当数据量不够“大”时，可能会出现计算资源浪费现象。


![共享内存实现快速归约](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616659374259-image.png)


### 数据量和共享内存的关系
1. `每个归约变量所需空间 = 所需最大总线程块数 * TPB * sizeof（float）`


2. `归约变量数 * TPB = 所需共享内存大小<= 每个线程块最大共享内存大小
   49152B / ( 1024 * 4B) = 12 `  （这是将所有归约变量同时归约！）
3. 本例中我使用了三次归约：
   `1024*1024*1024 = 1,073,741,824`（当前框架不变，理论可支持的数据量）
4. 但是实际达不到，因为需要考虑全局内存的大小。
   `设备用于保存主机上相关数据+设备用于存储中间数据<全局内存大小`

### 效率（12,000,000数据）
上述8个步骤的运行时间如下表所示：

| Step | Runtime (s) |
| ---- | ----------- |
| 2-8  | 0.3935s     |
| 3-8  | 0.0767s     |
| 4-8  | 0.0529s     |
| 8    | 0.0080s     |

其他人的对比结果：
![CUDA: 使用原子操作、CUDA(2)：使用Thrust（a cool library that provides STL）、CUDA(3)：使用类似本方法。](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616660226392-image.png)


- `归约变量数 * TPB = 所需共享内存大小 <= 每个线程块最大共享内存大小（49152/1024=48）`
- `所需最大总线程块数 * TPB * sizeof（float）= 每个归约变量所需空间`


![Reduction](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-3-25/1616660338557-image.png)

