---
layout: post
category: Decision tree
title: 【机器学习】决策树算法属性筛选度量标准
tagline: by Kanglei Zhou
tags: 
  - Machine learning
published: true
---

# 决策树算法属性筛选度量标准

[TOC]

在[信息论](https://zh.wikipedia.org/wiki/信息论)中，**熵**（entropy）是接收的每条消息中包含的信息的平均量，又被称为**信息熵**、**信源熵**、**平均自信息量**。这里，“消息” 代表来自分布或数据流中的事件、样本或特征。熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在[信息论](https://zh.wikipedia.org/wiki/信息论)里面，熵是对不确定性的测量。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。

## 信息熵

1948年，香农（Shannon）引入热力学中的熵概念来衡量信息量。**熵是各种可能性的熵以概率为权重进行加和**，每个可能性可以用 $\log \frac{1}{p} = - \log p$ 来表示。对于信息 $x$，其所含的信息量为


$$
\mathrm{I}(x) = \log \frac{1}{p(x)} = -\log p(x)
$$


其中，$p(x)$ 为信息 $x$ 发生的概率。

**信息熵是信息量的数学期望**，是信源发出信息前的先验不确定性，也称先验熵。对于随机变量 $X$，$x_1,x_2,\cdots,x_n$ 为来自总体的简单样本，其概率密度分布为


$$
\mathrm{P}(X = x_i) = p_i
$$


满足


$$
\sum_{i=1}^n p_i = 1
$$


则 $X$ 的熵定义为


$$
\mathrm{H}(X) = \mathrm{I}_E(x_1,x_2,\cdots,x_n) =  \sum_{i=1}^n p_i I(x_i) =  -\sum_{i=1}^np_i\log p_i
$$


设存在信源 $x_1,x_2$，下图为信息熵 $\mathrm{H}$ 随信源 $x_1$ 的发生概率 $P(x_1)$ 的变化图，当 $P(x_1) = 1$ 或  $P(x_2) = 0$ 时，则信息熵 $\mathrm{H} = 0$ 表示没有发送的不确定性。而当 $P(x_1) = 0.5$ 时，熵达到最大，不确定性最大。

![信息熵随信源 $x_1$ 概率变化图](https://img-blog.csdnimg.cn/20191106160037119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nub3dkcm9wdHVsaXA=,size_16,color_FFFFFF,t_70)

举例来说，如果一个分类系统中，类别的标识是 $c$，取值情况是 $c_1 , c_2 ,\cdots,c_n$，$n$ 为类别的总数。那么该分类系统的熵为：


$$
\mathrm{H}(c) = -\sum_{i=1}^np(c_i)\log p(c_i)
$$


特别地，如果是个二分类系统，那么此系统的熵为：


$$
\mathrm{H}(c) = -p(c_0)\log_2p(c_0) - p(c_1) \log p(c_1)
$$


其中 $p(c_0),p(c_1)$ 分别为两类样本出现的概率。

## 信息增益

决策树算法 [ID3](https://zh.wikipedia.org/w/index.php?title=ID3_algorithm&action=edit&redlink=1) 的生成使用[信息增益](https://zh.wikipedia.org/wiki/信息增益)。信息增益是基于[信息论](https://zh.wikipedia.org/wiki/信息论)中[信息熵](https://zh.wikipedia.org/wiki/信息熵)与[自信息](https://zh.wikipedia.org/wiki/自信息)理论，被定义为


$$
\begin{aligned}
\overbrace {\mathrm{IG}(T,a)} ^{\text{Information Gain}} &=\overbrace {\mathrm {H} (T)} ^{\text{Entropy (parent)}}-\overbrace {\mathrm {H} (T|a)} ^{\text{Weighted Sum of Entropy (Children)}}\\
&=-\sum _{i=1}^{J}p_{i}\log _{2}{p_{i}}-\sum _{a}{p(a)\sum _{i=1}^{J}-\Pr(i|a)\log _{2}{\Pr(i|a)}}
\end{aligned}
$$


来看一个 Wikipedia 中给出的例子：

> **例**	数据集有4个属性：*outlook* (sunny, overcast, rainy), *temperature* (hot, mild, cool), *humidity* (high, normal), 和 *windy* (true, false), 目标值*play*（yes, no）, 总共14个数据点。
>
> | **Outlook** | **Temperature** | **Humidity** | **Windy** | **Play?** |
> | :---------- | :-------------- | :----------- | :-------- | :-------- |
> | sunny       | hot             | high         | false     | no        |
> | sunny       | hot             | high         | true      | no        |
> | overcast    | hot             | high         | false     | yes       |
> | rainy       | mild            | high         | false     | yes       |
> | rainy       | cool            | normal       | false     | yes       |
> | rainy       | cool            | normal       | true      | no        |
> | overcast    | cool            | normal       | true      | yes       |
> | sunny       | mild            | high         | false     | no        |
> | sunny       | cool            | normal       | false     | yes       |
> | rainy       | mild            | normal       | false     | yes       |
> | sunny       | mild            | normal       | true      | yes       |
> | overcast    | mild            | high         | true      | yes       |
> | overcast    | hot             | normal       | false     | yes       |
> | rainy       | mild            | high         | true      | no        |
>
> 为建造决策树，需要比较4棵决策树的信息增益，每棵决策树用一种属性做划分。信息增益最高的划分作为第一次划分，并在每个子节点继续此过程，直至其信息增益为0。

若使用属性*windy*做划分时，产生2个子节点：*windy*值为真与为假。当前数据集，6个数据点的*windy*值为真，其中3个点的*play*值为真，3个点的*play*值为假；其余8个数据点的*windy*为假，其中6个点的*play*值为真，2个点的*play*值为假。 

- *windy*=true的子节点的信息熵计算为：

$$
{\displaystyle \mathrm{I}_{E}([3,3])=-{\frac {3}{6}}\log _{2}^{}{\frac {3}{6}}-{\frac {3}{6}}\log _{2}^{}{\frac {3}{6}}=-{\frac {1}{2}}\log _{2}^{}{\frac {1}{2}}-{\frac {1}{2}}\log _{2}^{}{\frac {1}{2}}=1}
$$

- *windy*=false的子节点的信息熵计算为：

$$
{\displaystyle \mathrm{I}_{E}([6,2])=-{\frac {6}{8}}\log _{2}^{}{\frac {6}{8}}-{\frac {2}{8}}\log _{2}^{}{\frac {2}{8}}=-{\frac {3}{4}}\log _{2}^{}{\frac {3}{4}}-{\frac {1}{4}}\log _{2}^{}{\frac {1}{4}}=0.8112781}
$$

这个划分（使用属性*windy*）的信息熵是两个子节点信息熵的加权和：


$$
{\displaystyle \mathrm{I}_{E}([3,3],[6,2])=\mathrm{I}_{E}({\text{windy or not}})={\frac {6}{14}}\cdot 1+{\frac {8}{14}}\cdot 0.8112781=0.8921589}
$$


为计算使用属性*windy*的信息增益，必须先计算出最初（未划分）的数据集的信息熵，数据集的*play*有9个yes与5个no：


$$
{\displaystyle \mathrm{I}_{E}([9,5])=-{\frac {9}{14}}\log _{2}^{}{\frac {9}{14}}-{\frac {5}{14}}\log _{2}{\frac {5}{14}}=0.940286}
$$


因此，使用属性*windy*的信息增益是：


$$
{\displaystyle \mathrm{IG}({\text{windy}})=\mathrm{I}_{E}([9,5])-\mathrm{I}_{E}([3,3],[6,2])=0.940286-0.8921589=0.0481271}
$$


上面只是举如果按照 Windy 划分算得的信息增益， 那么如果按照Temperature 、 Humidity、 Outlook 划分呢？同理，经计算


$$
\begin{aligned}
\mathrm{IG(outlook)} = 0.246 \\
\mathrm{IG(Temperature)} = 0.029 \\
\mathrm{IG(Humidity)} = 0.151
\end{aligned}
$$


选择最大的信息增益属性进行划分，然后再重复进行上述步骤，直至建好一棵树为止；在本例中第一个分支节点的属性是 Outlook。

> **注意**：取值更多的属性容易使得数据更“纯”，其信息增益更大。决策树会首先挑选这个属性作为树的顶/结点；结果训练出来的形状是一棵庞大且深度很浅的树，这样的划分极不合理。

## 信息增益率

由于信息增益偏好取值多的属性（极限趋近于均匀分布），所以决策树 C4.5 算法就采用增益率替代了 ID3 算法的信息增益。信息增益率定义为


$$
\mathrm{IG_{r}}= \frac{\mathrm{IG}(T,a)}{\mathrm{H}(a)}
$$


其中 $a$ 为的固有属性，$\mathrm{IG}(T,a)$ 为按照属性 $a$ 划分时的信息增益，$\mathrm{H}(a)$ 为按照属性 $a$ 划分的信息熵。

因此，按属性 Outlook 划分时，信息熵为


$$
{\displaystyle \mathrm{I}_{E}([5,4,5])=-{\frac {5}{14}}\log _{2}^{}{\frac {5}{14}}-{\frac {4}{14}}\log _{2}^{}{\frac {4}{14}}-{\frac {5}{14}}\log _{2}^{}{\frac {5}{14}}=1.5774}
$$


信息增益率为


$$
\mathrm{IG_{r}}(\mathrm{outlook}) = \frac{0.246}{1.5774} = 0.15595
$$



## 基尼系数

但是无论是 ID3 还是 C4.5，都是基于信息论的熵模型的，这里面会涉及大量的对数运算（为了避免这个问题）。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！CART（Classification and Regression Tree）分类树算法使用基尼系数来代替信息增益比，**基尼系数代表了模型的不纯度**，基尼系数越小，则不纯度越低（纯度越高），特征越好。

同样地，对于 $n$ 类别 $c_1,c_2,\cdots,c_n$ 样本 $T$，第 $i$ 个类别出现地概率为 $p_k$，则基尼系数为


$$
\mathrm{Gini}(T) = \sum_{i=1}^n p_i(1-p_i) = 1- \sum_{i=1}^n p_i^2
$$


特别地，对于二分类问题，其基尼系数为


$$
\mathrm{Gini}(T) = \sum_{i=1}^2 p_i(1-p_i) = 1- \sum_{i=1}^2 p_i^2 = 2p(1-p)
$$


当一个节点中所有样本都是一个类时，基尼不纯度为零。

如果特征 $a$ 有 $k$ 种类别，则其基尼系数为


$$
\mathrm{Gini}(T,a) = \sum_{i=1}^k \frac{|T^i|}{|T|} \mathrm{Gini}(T^i)
$$


在特征选取中，会优先选择基尼指数**最小**的属性作为优先划分属性。


$$
a^* = \mathop{\arg \min}_{a \in A} \mathrm{Gini}(T,a)
$$

## 参考资料

[1]	[超详细的信息熵、信息增益、信息增益比、基尼系数](https://blog.csdn.net/snowdroptulip/article/details/102935227)

[2]	[信息增益_信息增益率_Gini, xiaoxiyouran](https://xiaoxiyouran.xyz/blogger/docs/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/20180827%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A_%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87_Gini/20180827%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A_%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87_Gini.html)

[3]	[决策树学习, Wikipedia](https://zh.wikipedia.org/wiki/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A)

