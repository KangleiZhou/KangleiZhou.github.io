---
layout: post
category: NLP
title: 【深度学习】学习笔记
tagline: by Kanglei Zhou
author: Kanglei Zhou
tags: 
  - NLP
published: true
---



> 推导高斯模型的二元算法



# 手写推导

![第一页](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-4-22/1619078340416-1.jpg)

![第二页](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-4-22/1619078367370-2.jpg)

以下节选自知乎文章，[高斯混合模型(GMM)与EM算法的推导](https://zhuanlan.zhihu.com/p/71010421 "GMM-EM算法推导")。

# 高斯分布

高斯分布是拟合随机数据最常用的模型。单变量 $
x
$ 的高斯分布概率密函数如下:
$$
\begin{equation*} \textit{N}(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \end{equation*}
$$

其中

  * $\mu$ 分布的数学期望，
  * $\sigma$ 标准差，
  * $\sigma ^{2}$ 是方差。

更一般的情况，如果数据集是 $d$ 维的数据, 就可以用多变量高斯模型来拟合。概率密度是: 

$$
\begin{equation*} \textit{N}(x; \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma)}}\exp\left[-\frac{1}{2}(x-\mu)\Sigma^{-1}(x-\mu)^T\right] \end{equation*}
$$
其中

  * $x$ 是一个 $N \times d$ 的向量, 代表 $N$ 组 $d$ 维数据, 
  * $\mu$ 是一个 $1\times d$ 的向量, 代表每维的数学期望, 
  * $\Sigma$ 是一个 $d\times d$ 的矩阵, 代表模型的协方差矩阵

# 高斯混合模型GMM

现实采集的数据是比较复杂的，通常无法只用一个高斯分布拟合，而是可以看作多个随机过程的混合。可定义高斯混合模型是 $K$ 个高斯分布的组合，用以拟合复杂数据。

假设有一个数据集，包含了 $N$ 个相互独立的数据：$x = {x_1, x_2 ...x_i... x_N}$, 这些数据看起来有 $K$个峰，这样的数据集可用以下定义的高斯混合模型拟合：
$$
\begin{equation*} p(x|\Theta) = \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \sigma_k) = \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{2\pi\sigma_k^2}}\exp\left[-\frac{(x-\mu_k)^2}{2\sigma_k^2}\right] \end{equation*}
$$
单变量GMM概率密度分布直方图![如果每一个数据点 $x_i$ 都是 $d$ 维的, 这些数据 $x$ 看起来分散在 $K$ 个聚类，这种数据集可以用多变量高斯混合模型拟合。
$$
\begin{eqnarray} p(x|\Theta) &=& \sum_{k}^{}\alpha_{k}\textit{N}(x; \mu_k, \Sigma_k) \nonumber \\&=& \sum_{k}^{}\alpha_{k}\frac{1}{\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x-\mu_k)^T\Sigma_{k}^{-1}(x-\mu_k)\right] \end{eqnarray}
$$
其中 $\Theta$ 代表全体高斯模型参数, $\alpha_k$ 是第 $k$ 个高斯模型的先验概率, 各个高斯模型的先验概率加起来等于1。
$$
\begin{equation*} \sum_{k}\alpha_k = 1 \end{equation*}
$$
# EM算法

EM 算法是一种迭代的算法，算法解决的问题可如下表述：

  * 采集到一组包含 $N$ 个独立数据的数据集 $x$ 。
  * 预先知道、或者根据数据特点估计可以用 $K$ 个高斯分布混合进行数据拟合。
  * 目标任务是估计出高斯混合模型的参数： $K$ 组 $(\alpha_{k},   \mu_k, \sigma_k)$ , 或者 $(\alpha_{k}, \mu_k, \Sigma_{k})$ 。

## EM算法解多变量GMM

我们可以得到每次迭代的目标函数如下：

$$
\begin{equation*} Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\ln\frac{\alpha_{k}}{\omega_{i,k}^t\sqrt{(2\pi)^d\det(\Sigma_k)}}\exp\left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right] \end{equation*}
$$

其中

  * $x_i$ 是 $1\times d$ 的向量, 
  * $\alpha_k$ 是一个0和1间的值, 
  * $\mu_k$ 是 $1\times d$的向量,
  * $\Sigma_k$ 是 $d\times d$的矩阵, 
  * $\omega$ 是 $N \times K$ 的矩阵。

## E-Step:

跟单变量 GMM 一样，E-step 计算隐参数，但是需要用多维高斯分布，利用了多维矩阵乘法和矩阵求逆，计算复杂度要大很多。
$$
\begin{equation*} \omega_{i,k}^t = \frac{\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)}{\sum_{k}\alpha_{k}^t\textit{N}(x_{i}| \mu_k^t, \Sigma_k^t)} \end{equation*}
$$
目标函数更新如下：

$
Q(\Theta,\Theta^{t}) = \sum_{i}\sum_{k}\omega_{i,k}^t\left(\ln\alpha_k - \ln \omega_{i,k}^t - \frac{d}{2}\ln \sqrt{(2\pi)^d} -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right)
$

## M-Step:

- **更新** $  \alpha_{k}  $**:**
多变量GMM下， $\alpha_k$ 的更新跟单变量GMM一样。
$$
\begin{gather} \alpha_k^{t+1} := \mathop{\arg\max}_{\alpha_k}{ \sum_{i}\sum_{k}\omega_{i,k}^t\ln\alpha_k}\\ \text{subject to} \sum_{k}\alpha_k =1 \nonumber \end{gather}
$$
得到完全一样的更新方程：
$$
\begin{equation*} \alpha_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t}{N} \end{equation*}
$$
- **更新** $  \mu_k  $：

$$
\begin{equation*} \mu_k^{t+1} :=  \mathop{\arg\max}_{\mu_k} Q(\Theta,\Theta^{t}) \end{equation*}
$$
$Q(\Theta,\Theta^{t})$ 对 $\mu_k$ 求导，得到
$$
\begin{eqnarray} \frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} = \sum_{i}\omega_{i,k}^t\frac{\partial \left[-\frac{1}{2}(x_i-\mu_k)^T\Sigma_k^{-1}(x_i-\mu_k)\right]}{\partial \mu_k}  = 0\ \end{eqnarray}
$$
实数协方差矩阵 $\Sigma_{k}$对称的, 其逆矩阵也是对称的。 于是我们可以利用第一部分列出的公式 
$$
\frac{\partial (x -s)^TW(x-s)}{\partial x} = -2W(x-s)

$$
求偏导数。

$$
\begin{equation*} \frac{\partial Q(\Theta,\Theta^{t})}{\partial \mu_k} =   \sum_{i}\omega_{i,k}^t \Sigma_{k}^{-1}\left(x_i - \mu_k\right) =0 \end{equation*}
$$

$$
\begin{equation*} \Rightarrow \sum_{i}\omega_{i,k}^t x_i= \mu_k\sum_{i}\omega_{i,k}^t \end{equation*}
$$
所以 $\mu_k$ 的更新方程同样是 $x$ 的加权平均，只是这时候 $\mu_k$ 是一个 $1\times d$ 向量。


$$
\begin{equation*}  \mu_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t x_i}{\sum_{i}\omega_{i,k}^t} \end{equation*}
$$


- **更新**$  \Sigma_k  $ **:**


$$
\begin{equation*} \Sigma_k^{t+1} :=  \mathop{\arg\max}_{\Sigma_k} Q(\Theta,\Theta^{t}) \end{equation*}
$$


让导数 $\frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =0$, 得到

$$
 \begin{eqnarray} \frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} &=& \sum_{i}\omega_{i,k}^t\frac{\partial \left[ -\frac{1}{2}\ln\det(\Sigma_k)-\frac{1}{2}(x_i-\mu_k)\Sigma_k^{-1}(x_i-\mu_k)^T\right]}{\partial \Sigma_k^{-1}}  \nonumber\\  &=& -\frac{1}{2} \sum_{i}\omega_{i,k}^t \left[\frac{\partial \ln\det(\Sigma_k)}{\partial \Sigma_k^{-1}}+\frac{\partial (x_i-\mu_k)\Sigma_k^{-1}(x_i-\mu_k)^T}{\partial  \Sigma_k^{-1}} \right] \\  &=& 0 \nonumber \end{eqnarray} 
$$

协方差矩阵 $\Sigma_k$是对称的，可以利用第一部分的矩阵求导公式

$$
\frac{\partial \ln \det(X)}{\partial X^{-1}} =-X^T
$$

和
$$
\frac{\partial a^TXa}{\partial X} = aa^T
$$

求得极大值 $Q(\Theta,\Theta^{t})$。

$$
\begin{equation*} \frac{\partial Q(\Theta,\Theta^{t})}{\partial \Sigma_k^{-1}} =   \frac{1}{2}\sum_{i}\omega_{i,k}^t \left[\Sigma_k - (x_i-\mu_k)^T(x_i-\mu_k)\right] = 0 \end{equation*}
$$

类似地, 我们可以得到 $\Sigma_k$ 在第 $t+1$ 次迭代的更新方程, 它依赖于$\mu_k$ 。所以我们需要先计算 $\mu_k^{t+1}$，然后更新 $\Sigma_k$

$$
\begin{equation*} \Sigma_k^{t+1} = \frac{\sum_{i}\omega_{i,k}^t (x_i-\mu_k^{t+1})^T(x_i-\mu_k^{t+1}) }{\sum_{i}\omega_{i,k}^t} \end{equation*}
$$

# 总结

![](https://pic3.zhimg.com/v2-5211017955690004a305df9bcc782c86_b.jpg)

# 代码实现

GMM-EM 算法 E 步的实现和测试代码如下，完整代码见 [Here](https://towardsdatascience.com/gaussian-mixture-modelling-gmm-833c88587c7f "Gaussian Mixture Modelling (GMM)")

```python
      def _e_step(self, X, pi, mu, sigma):
        """Performs E-step on GMM model
        Parameters:
        ------------
        X: (N x d), data points, m: no of features
        pi: (C), weights of mixture components
        mu: (C x d), mixture component means
        sigma: (C x d x d), mixture component covariance matrices
        Returns:
        ----------
        gamma: (N x C), probabilities of clusters for objects
        """
        N = X.shape[0] 
        self.gamma = np.zeros((N, self.C))

        const_c = np.zeros(self.C)
        
        
        self.mu = self.mu if self._initial_means is None else self._initial_means
        self.pi = self.pi if self._initial_pi is None else self._initial_pi
        self.sigma = self.sigma if self._initial_cov is None else self._initial_cov

        for c in range(self.C):
            # Posterior Distribution using Bayes Rule
            self.gamma[:,c] = self.pi[c] * mvn.pdf(X, self.mu[c,:], self.sigma[c])

        # normalize across columns to make a valid probability
        gamma_norm = np.sum(self.gamma, axis=1)[:,np.newaxis]
        self.gamma /= gamma_norm

        return self.gamma
```

