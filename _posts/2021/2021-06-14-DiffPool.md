---
layout: post
category: GNN
title: 【GNN】Hierarchical Graph Representation Learning with DiffPool
tagline: by Kanglei Zhou
author: Kanglei Zhou
tags: 
  - GNN
published: true
---

> 现有的 GNN 方法由于其存在平面化的局限性，因此无法学习图的层级表示。让 GNN 能够学习到图的层次表示对于图表示学习极为重要，例如，图分类的目标是预测与整个图相关的标签。标准的方法是为图中的所有节点生成嵌入，然后将所有这些节点嵌入全局集中在一起，例如，使用一个简单的求和或对集合进行池化操作的神经网络。这种全局池方法忽略了图中可能存在的层次结构，阻碍了在整个图上为预测任务建立有效的 GNN 模型。

研究者们出于这个目的，提出了一个可微图池模块 `DiffPool`，可以生成图的层次表示，并可以端到端地与各种图神经网络体系结构结合。



![High-level illustration of our proposed method `DiffPool`](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-14/1623635713437-image.png)

# Graph classification & GNNs

定义图 $G = (\mathbf{A}, \mathbf{F})$，其中 $\mathbf{A} \in \mathbb{R}^{n\times n}$ 是邻接矩阵，$\mathbf{F} \in \mathbb{R}^{n\times d}$ 是节点的特征矩阵。给定一个图集合 $\mathcal{D} = \{(G_1, y_1), (G_2, y_2), \cdots, (G_N, y_N)\}$，其中 $y_i \in \mathcal{Y}$ 是图 $G_i$ 的类别标签。图分类任务就是学习一个图到标签的映射 $f: \mathcal{G} \to \mathcal{Y}$，将图映射到标签集。

通过线性变换和 `ReLU` 非线性的组合，图卷积操作可以定义为：


$$
\mathbf{H}^{(k)} = \mathrm{ReLU}(\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{H}^{(k - 1)} \mathbf{W}^{(k - 1)})
$$


其中，$\mathbf{W}^{(k - 1)} \in \mathbb{R}^{d\times d}$ 是可学习参数矩阵。

GNN 模块根据邻接矩阵 $\mathbf{A}$ 和初始输入节点特征 $\mathbf{X}$ 实现消息传递的 $K$ 次迭代可以表示为


$$
\mathbf{Z} = \mathrm{GNN}(\mathbf{A}, \mathbf{X}) \in \mathbb{R}^{n \times d}
$$


试图定义一个策略来输出一个包含 $m < n$ 个节点的粗图，新的邻接矩阵 $\mathbf{A}' \in \mathbb{R}^{m \times m}$，新的节点嵌入 $\mathbf{Z}' \in \mathbb{R}^{m \times d}$。即，需要模型学习一种池化策略，它将泛化具有不同节点和边的图，并在推理过程中适应不同的图结构。

# Differentiable Pooling via Learned Assignments

将学习到的第 $l$ 层簇分配矩阵表示为 $\mathbf{S}^{(l)} \in\mathbb{R}^{n_l \times n_{l+1}}$，$\mathbf{S}^{(l)}$ 的每一行对应于第 $l$ 层的$n_l$ 个节点，以及 $\mathbf{S}^{(l)}$ 的每一列对应于下一层 $l+1$ 的 $n_{l+1}$ 簇。

## Learning the assignment matrix

使用两个单独的 GNN 模块生成嵌入矩阵 $\mathbf{Z}^{(l)}$ 和分配矩阵 $\mathbf{S}^{(l)}$，它们都应由输入簇节点特征 $\mathbf{X}^{(l)}$ 和邻接矩阵 $\mathbf{A}^{(l)}$ 生成。


$$
\begin{align*}
		\mathbf{Z}^{(l)} &= \mathrm{GNN}_{l,\mathrm{embed}} \left(\mathbf{A}^{(l)}, \mathbf{X}^{(l)}\right)\\
		\mathbf{S}^{(l)} &= \mathrm{softmax}\left (\mathrm{GNN}_{l,\mathrm{pool}} \left(\mathbf{A}^{(l)}, \mathbf{X}^{(l)}\right)\right)
	\end{align*}
$$


其中，`softmax` 函数以行方式执行。

注意，这两个 GNN 使用相同的输入数据和不同的参数，发挥不同的作用：嵌入的 GNN 在这一层为输入节点生成新的嵌入，而池化的 GNN 生成输入节点到 $n_{l+1}$ 簇的概率分配。

## Pooling with an assignment matrix

由于簇分配矩阵 $\mathbf{S}^{(l)}$ 已经计算得到，即我们已经计算了模型的第 $l$ 层的赋值矩阵。于是，接下来为每个粗图节点/簇生成一个新的粗化邻接矩阵 $\mathbf{A}^{(l+1)}$ 和一个新的嵌入矩阵 $\mathbf{X}^{(l+1)}$ 。


$$
\begin{align*}
		\mathbf{X}^{(l + 1)} &= {\mathbf{S}^{(l)}}^\top \mathbf{Z}^{(l)} \in \mathbb{R}^{n_{l+1} \times d} \\
		\mathbf{A}^{(l + 1)} &= {\mathbf{S}^{(l)}}^\top \mathbf{A}^{(l)} {\mathbf{S}^{(l)}} \in \mathbb{R}^{n_{l+1} \times n_{l+1}}
	\end{align*}
$$


取节点嵌入 $\mathbf{Z}^{(l)}$，根据聚类分配矩阵 $\mathbf{S}^{(l)}$ 聚合这些嵌入，最终为每个 $n_{l+1}$ 集群生成新的嵌入 $\mathbf{X}^{(l + 1)}$。同样，取邻接矩阵 $\mathbf{A}^{(l)}$，生成一个粗邻接矩阵表示每对簇之间的连通性强度。

上述过程可以被形式化记为：


$$
\mathbf{A}^{(l + 1)}, \mathbf{X}^{(l + 1)} = \mathrm{DiffPool}(\mathbf{A}^{(l)}, \mathbf{X}^{(l)}) 
$$



## Permutation invariance

注意，为了便于图分类，池化层在节点排列下应该是不变的。对于 `DiffPool`，得到以下性质，表明任何基于 `DiffPool` 的深度 GNN 模型都是置换不变的，只要所使用的 GNN 是置换不变的。

> **性质 1**：令 $\mathbf{P} \in \{0, 1\}^{n \times n}$ 为任意置换矩阵，当且仅当 $\mathrm{GNN}(\mathbf{A}, \mathbf{X}) = \mathrm{GNN} (\mathbf{P}^\top\mathbf{A}\mathbf{P})$ 时，有 $\mathrm{DiffPool}(\mathbf{A}, \mathbf{X}) = \mathrm{DiffPool} (\mathbf{P}^\top\mathbf{A}\mathbf{P})$。

由于置换矩阵是正交的，即 $\mathbf{P}^\top \mathbf{P} = \mathbf{I}$，若 $\mathrm{GNN}(\mathbf{A}, \mathbf{X}) = \mathrm{GNN} (\mathbf{P}^\top\mathbf{A}\mathbf{P})$ 成立，显然有 $\mathrm{DiffPool}(\mathbf{A}, \mathbf{X}) = \mathrm{DiffPool} (\mathbf{P}^\top\mathbf{A}\mathbf{P})$ 成立。

# Auxiliary Link Prediction Objective and Entropy Regularization

在实践中，仅使用图分类任务中的梯度信号很难训练池化 GNN。直观地说，我们有一个非凸优化问题，在训练初期很难将池化的 GNN 从伪局部极小值中推出来。为了缓解这个问题，用一个**辅助链接预测目标**来训练池化 GNN，该目标对邻近节点应该被池化的直觉进行编码。

在每一层中，最小化目标


$$
\min ~~ L_{\mathrm{LP}} = \left\|\mathbf{A}^{(l)}, \mathbf{S}^{(l)} {\mathbf{S}^{(l)}}^\top \right\|_F
$$


其中，$\|\cdot\|_F$ 为  Frobenius 范数。

每个节点的输出聚类分配通常应该接近一个独热向量，这样每个聚类或子图的隶属关系就可以明确定义。为此，通过最小化来正则簇分配的熵


$$
L_{\mathrm{E}} = \frac{1}{n} \sum_{i = 1}^n H(\pmb{S}_i)
$$


其中 $H$ 表示熵函数，$\pmb{S}_i$ 是 $\bf S$ 的第 $i$ 行。

在训练过程中，每一层的 $L_{\mathrm{LP}}$ 和 $L_{\mathrm{E}}$ 被添加到分类损失中。在实践中，我们观察到，带有次要目标的训练需要更长的时间来集中，但却能获得更好的表现和更可解释的群集分配。

# Experiments

下表比较了 `DiffPool` 和一些最先进的图分类基准的性能。`DiffPool` 方法在所有方法中获得了最高的平均性能。

![Classification accuracies in percent](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-14/1623641438029-image.png)



将 `DiffPool` 用在 S2V 中，分类准确率方面的结果总结在下表中。易知，`DiffPool` 显著提高了 S2V 在酶和D&D数据集上的性能。

![](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-14/1623641602258-image.png)



通过单独池化稀疏子图的不同部分，`DiffPool` 可以学习捕捉稀疏图区域中存在的有意义的结构。

![Visualization of hierarchical cluster assignment in `DiffPool`, using example graphs from COLLAB.](https://cdn.jsdelivr.net/gh/ZhouKanglei/jidianxia/2021-6-14/1623641749141-image.png)

# 小结

为 GNN 引入了一种可微分池方法，该方法能够提取图的复杂层次结构。通过将池化层应用在现有的在 GNN 模型中，在几个图分类基准上取得了最新的结果。未来发展方向包括学习硬集群分配（预定义簇分配矩阵），以进一步降低更高层次的计算成本，同时确保可差异化，并将分层池方法应用于其他需要对整个图结构建模的下游任务。